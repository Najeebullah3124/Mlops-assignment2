# Assignment 02 - Complete Checklist

**Deadline: 14-12-2025**

## âœ… Task 1 - Project Setup + Version Control (Git + DVC)

### 1.1 Create Project Structure âœ…
- [x] Git initialized
- [x] .gitignore created
- [x] Python environment (requirements.txt)
- [x] Project structure created
- [ ] **Screenshot**: Folder structure
- [ ] **Screenshot**: git init output

### 1.2 Initialize DVC âœ…
**Commands Executed:**
```bash
âœ… dvc init
âœ… dvc remote add -d myremote ./dvcstore
âœ… dvc add data/dataset.csv
âœ… git add data/dataset.csv.dvc .dvc .dvcignore
âœ… git commit -m "Add dataset with DVC"
```

**Files Created:**
- âœ… `.dvc/` directory
- âœ… `data/dataset.csv.dvc`
- âœ… `.dvcignore`

- [ ] **Screenshot**: DVC commands output
- [ ] **Screenshot**: `.dvc` file (data/dataset.csv.dvc)
- [ ] **Screenshot**: DVC status/output logs

### 1.3 Create Training Pipeline âœ…
**Commands Executed:**
```bash
âœ… dvc stage add -n train_model -d src/train.py -d data/dataset.csv -o models/model.pkl python3 src/train.py
âœ… dvc repro train_model
```

**Files Created:**
- âœ… `dvc.yaml`
- âœ… `dvc.lock`

**Pipeline Executed:**
- âœ… Model trained successfully
- âœ… MSE: 0.0097, RÂ²: 0.9985

- [ ] **Screenshot**: Successful DVC pipeline run
- [ ] **Screenshot**: dvc.yaml file

---

## âœ… Task 2 - CI/CD Pipeline (GitHub Actions)

### 2.1 Create Workflow File âœ…
**File Created:**
- âœ… `.github/workflows/ci.yml`

**Workflow Includes:**
- âœ… Setup Python
- âœ… Install requirements
- âœ… Run unit tests
- âœ… Run linting (flake8, pylint)
- âœ… Verify training script runs

- [ ] **Screenshot**: Workflow file
- [ ] **Screenshot**: GitHub Actions successful run
- [ ] **Screenshot**: (Optional) Failing workflow

**Action Needed:**
- [ ] Push code to GitHub repository
- [ ] Screenshot workflow run

### 2.2 Add Tests âœ…
**File Created:**
- âœ… `tests/test_train.py`

**Tests Include:**
- âœ… Data loading (4 tests)
- âœ… Model training (3 tests)
- âœ… Shape validation (2 tests)
- âœ… Model evaluation (4 tests)
- âœ… Data preparation (3 tests)

**Total: 16 tests, all passing**

- [ ] **Screenshot**: Test results

---

## âœ… Task 3 - Docker

### 3.1 Create Dockerfile âœ…
**File Created:**
- âœ… `Dockerfile`

**Commands Executed:**
```bash
âœ… docker build -t mlops-app .
âœ… docker run mlops-app
```

**Results:**
- âœ… Image built successfully (1.12GB)
- âœ… Container runs successfully
- âœ… Training completes in container

- [ ] **Screenshot**: Dockerfile
- [ ] **Screenshot**: Build logs
- [ ] **Screenshot**: Running container

### 3.2 Push Docker Image to Docker Hub âš ï¸
**Action Needed:**
```bash
# Replace 'yourusername' with your Docker Hub username
docker tag mlops-app yourusername/mlops-app:v1
docker login
docker push yourusername/mlops-app:v1
```

- [ ] **Screenshot**: Docker Hub repository page
- [ ] **Screenshot**: Push logs

---

## âœ… Task 4 - Airflow Pipeline

### 4.1 Install Airflow Locally âœ…
**Files Created:**
- âœ… `docker-compose.yaml`
- âœ… `airflow/dags/train_pipeline.py`
- âœ… `.env` (AIRFLOW_UID=501)

**Commands Executed:**
```bash
âœ… docker compose up airflow-init
âœ… docker compose up -d
```

**Services Running:**
- âœ… PostgreSQL (healthy)
- âœ… Airflow Webserver (port 8080)
- âœ… Airflow Scheduler

**DAG Created:**
- âœ… `train_pipeline` with 4 tasks:
  1. load_data
  2. train_model
  3. save_model
  4. log_results

**Access:**
- URL: http://localhost:8080
- Username: airflow
- Password: airflow

- [ ] **Screenshot**: Airflow UI
- [ ] **Screenshot**: DAG graph
- [ ] **Screenshot**: Successful job run

**Action Needed:**
- [ ] Access Airflow UI
- [ ] Trigger DAG
- [ ] Capture screenshots

---

## âœ… Task 5 - RESTful API

### 5.1 Build ML Inference API âœ…
**File Created:**
- âœ… `api/app.py` (Flask API)

**Endpoints:**
- âœ… `/` - API information
- âœ… `/health` - Health check
- âœ… `/predict` - Model prediction (POST)
- âœ… `/predict/batch` - Batch prediction (POST)

**API Deployed:**
- âœ… Running on EC2: http://13.60.67.119:8000

- [ ] **Screenshot**: API running
- [ ] **Screenshot**: Testing using Postman/cURL
- [ ] **Screenshot**: Sample prediction outputs

### 5.2 Containerize the API âœ…
**Files Created:**
- âœ… `api/Dockerfile`
- âœ… `api/requirements.txt`

**Commands Executed:**
```bash
âœ… docker build -t mlops-api:v1 (on EC2)
âœ… docker run -d -p 8000:8000 mlops-api:v1 (on EC2)
```

**Status:**
- âœ… API containerized
- âœ… Running on EC2 port 8000

- [ ] **Screenshot**: API running in Docker

---

## âœ… Task 6 - AWS EC2 + S3 Deployment

### 6.1 Create AWS S3 Bucket âœ…
**Bucket Details:**
- âœ… Name: `mlops-test-ass`
- âœ… Region: eu-north-1
- âœ… Dataset uploaded: `dataset.csv` (114.7 KB)
- âœ… URL: https://mlops-test-ass.s3.eu-north-1.amazonaws.com/dataset.csv

- [ ] **Screenshot**: S3 bucket
- [ ] **Screenshot**: Uploaded data

### 6.2 Launch EC2 Instance âœ…
**Instance Details:**
- âœ… Public IP: 13.60.67.119
- âœ… OS: Ubuntu 22.04
- âœ… Port 8000: Open
- âœ… Port 22 (SSH): Open

**Dependencies Installed:**
- âœ… Docker
- âœ… Python3 & pip3
- âœ… AWS CLI

- [ ] **Screenshot**: EC2 instance dashboard
- [ ] **Screenshot**: Instance SSH terminal

### 6.3 Deploy API Using Docker on EC2 âœ…
**Deployment:**
- âœ… API image built on EC2
- âœ… Container running: `mlops-api`
- âœ… Port 8000 mapped
- âœ… API accessible: http://13.60.67.119:8000

**Commands Executed:**
```bash
âœ… docker build -t mlops-api:v1 (on EC2)
âœ… docker run -d -p 8000:8000 --name mlops-api mlops-api:v1 (on EC2)
```

- [ ] **Screenshot**: Logs
- [ ] **Screenshot**: Running container
- [ ] **Screenshot**: Public endpoint test (browser/Postman)

---

## ğŸ“Š Completion Status

| Task | Code | Deployment | Screenshots |
|------|------|------------|-------------|
| 1.1 Project Structure | âœ… 100% | âœ… 100% | âš ï¸ Need |
| 1.2 DVC Init | âœ… 100% | âœ… 100% | âš ï¸ Need |
| 1.3 DVC Pipeline | âœ… 100% | âœ… 100% | âš ï¸ Need |
| 2.1 Workflow File | âœ… 100% | âš ï¸ Need GitHub | âš ï¸ Need |
| 2.2 Tests | âœ… 100% | âœ… 100% | âš ï¸ Need |
| 3.1 Dockerfile | âœ… 100% | âœ… 100% | âš ï¸ Need |
| 3.2 Docker Hub | âœ… 100% | âš ï¸ Need Push | âš ï¸ Need |
| 4.1 Airflow | âœ… 100% | âœ… 100% | âš ï¸ Need |
| 5.1 API | âœ… 100% | âœ… 100% | âš ï¸ Need |
| 5.2 Containerize | âœ… 100% | âœ… 100% | âš ï¸ Need |
| 6.1 S3 | âœ… 100% | âœ… 100% | âš ï¸ Need |
| 6.2 EC2 | âœ… 100% | âœ… 100% | âš ï¸ Need |
| 6.3 Deploy | âœ… 100% | âœ… 100% | âš ï¸ Need |

**Code & Deployment: ~95% Complete**  
**Screenshots: Need to capture**

---

## ğŸ“ Submission Requirements

### PDF/Word Report Needs:
- [x] Explanation + Commands used (all documented)
- [x] Terminal logs (can be captured)
- [ ] Screenshots of outputs (need to capture)
- [ ] GitHub repository link (need to create/push)
- [x] AWS public endpoint URL: http://13.60.67.119:8000

### Zipped Project Folder:
- [x] All source code ready
- [x] Configuration files ready
- [x] Documentation ready
- [ ] Need to exclude large files (.git, __pycache__, .dvc/cache, etc.)

---

## ğŸ¯ What's Complete

### âœ… 100% Complete:
1. **DVC Setup** - All commands executed, pipeline working
2. **Docker Build & Run** - Image built, container tested
3. **Airflow Setup** - Services running, DAG ready
4. **API Development** - Created and deployed
5. **EC2 Deployment** - Instance configured, API running
6. **S3 Setup** - Bucket created, dataset uploaded

### âš ï¸ Needs Action:
1. **GitHub Push** - Push code to GitHub (10 min)
2. **Docker Hub Push** - Tag and push image (10 min)
3. **Screenshots** - Capture all required screenshots (1 hour)
4. **Report Writing** - Compile report with screenshots (1-2 hours)

---

## âœ… Summary

**Code & Deployment: 95% Complete** âœ…  
**Screenshots & Documentation: Need to capture** âš ï¸

**Everything is built and working!** You just need to:
1. Push to GitHub
2. Push to Docker Hub
3. Capture screenshots
4. Write the report

**Estimated time to complete: 2-3 hours**

All the hard work is done! ğŸ‰

