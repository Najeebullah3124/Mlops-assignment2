# Task 4 - Airflow Pipeline Summary

## âœ… Completed Setup

### 4.1 Airflow Installation with Docker Compose

**All Required Files Created:**

1. **docker-compose.yaml** âœ…
   - PostgreSQL database for Airflow metadata
   - Airflow webserver (UI on port 8080)
   - Airflow scheduler
   - Airflow init service
   - Volume mounts for dags, logs, plugins, config
   - Mounts for data and models directories

2. **airflow/Dockerfile** âœ…
   - Based on Apache Airflow 2.9.0
   - Installs system dependencies (gcc)
   - Installs Python requirements

3. **airflow/dags/train_pipeline.py** âœ…
   - Complete DAG with 4 tasks
   - All required functionality implemented

4. **Setup Scripts** âœ…
   - `airflow/setup_airflow.sh` - Automated setup
   - Environment configuration

### 4.2 DAG: train_pipeline.py

**DAG Structure:**
```
load_data â†’ train_model â†’ save_model â†’ log_results
```

**Task Details:**

#### Task 1: load_data
- âœ… Loads dataset from CSV file
- âœ… Creates dataset if it doesn't exist
- âœ… Validates dataset structure
- âœ… Logs dataset statistics
- âœ… Pushes data info to XCom for next tasks

#### Task 2: train_model
- âœ… Loads data from previous task
- âœ… Splits into train/test sets (80/20)
- âœ… Trains Linear Regression model
- âœ… Evaluates model (MSE, RÂ² score)
- âœ… Logs model performance metrics
- âœ… Pushes metrics to XCom

#### Task 3: save_model
- âœ… Creates models directory if needed
- âœ… Saves trained model as pickle file
- âœ… Verifies model was saved successfully
- âœ… Logs file size
- âœ… Pushes save status to XCom

#### Task 4: log_results
- âœ… Collects all metrics from previous tasks
- âœ… Logs comprehensive summary
- âœ… Saves summary report to JSON file
- âœ… Provides execution summary

## Setup Instructions

### Quick Start (3 Commands)

```bash
# 1. Set environment variable
echo -e "AIRFLOW_UID=$(id -u)" > .env

# 2. Initialize Airflow
docker compose up airflow-init

# 3. Start Airflow
docker compose up -d
```

### Access Airflow UI

1. Open browser: **http://localhost:8080**
2. Login:
   - Username: `airflow`
   - Password: `airflow`

### Run the Pipeline

1. In Airflow UI, find **`train_pipeline`** DAG
2. Toggle it **ON** (switch on the left)
3. Click on the DAG name
4. Click **"Trigger DAG"** button (play icon)
5. Watch execution in **Graph View**

## Screenshots Checklist

### Required Screenshots:

1. **Airflow UI Screenshot**
   - Main DAGs page showing `train_pipeline` DAG
   - DAG should be visible and enabled
   - Show the DAG list view

2. **DAG Graph Screenshot**
   - Click on `train_pipeline` DAG
   - Go to "Graph" view
   - Show all 4 tasks connected:
     - load_data
     - train_model
     - save_model
     - log_results
   - All tasks should show green (success) status

3. **Successful Job Run Screenshot**
   - After triggering the DAG
   - Show "Tree" or "Graph" view
   - All tasks completed successfully (green)
   - Show execution times
   - Can also show task logs

## Project Structure

```
Mlops-Project/
â”œâ”€â”€ docker-compose.yaml          # Airflow services configuration
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ Dockerfile              # Custom Airflow image
â”‚   â”œâ”€â”€ setup_airflow.sh        # Setup automation script
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ train_pipeline.py  # âœ… Training pipeline DAG
â”‚   â”œâ”€â”€ logs/                   # Airflow execution logs
â”‚   â”œâ”€â”€ plugins/                # Airflow plugins (empty)
â”‚   â””â”€â”€ config/                 # Airflow config (empty)
â”œâ”€â”€ data/                       # Dataset directory (mounted)
â”œâ”€â”€ models/                     # Model output (mounted)
â””â”€â”€ src/                        # Source code (mounted)
```

## DAG Features

### âœ… All Requirements Met:

- **Loads data** - Task 1: `load_data`
- **Trains model** - Task 2: `train_model`
- **Saves trained model** - Task 3: `save_model`
- **Logs results** - Task 4: `log_results`

### Additional Features:

- Comprehensive logging at each step
- Error handling and validation
- Data passing between tasks via XCom
- Model performance metrics tracking
- Summary report generation
- Automatic dataset creation if missing

## Useful Commands

### Start/Stop
```bash
# Start Airflow
docker compose up -d

# Stop Airflow
docker compose down

# View logs
docker compose logs -f
```

### Troubleshooting
```bash
# Check DAG syntax
python3 -m py_compile airflow/dags/train_pipeline.py

# View scheduler logs
docker compose logs airflow-scheduler

# Restart services
docker compose restart
```

## Verification Steps

Before taking screenshots, verify:

- [ ] Airflow UI is accessible at http://localhost:8080
- [ ] Can login with airflow/airflow
- [ ] `train_pipeline` DAG appears in the UI
- [ ] DAG can be enabled (toggle switch works)
- [ ] DAG can be triggered
- [ ] All 4 tasks execute successfully
- [ ] Model file is created in models/ directory
- [ ] Logs show detailed execution information

## Next Steps

1. **Start Airflow**: `docker compose up -d`
2. **Access UI**: http://localhost:8080
3. **Enable DAG**: Toggle `train_pipeline` ON
4. **Trigger DAG**: Click "Trigger DAG" button
5. **Monitor**: Watch execution in Graph View
6. **Screenshot**: Capture required screenshots

## Documentation Files

- `TASK4_AIRFLOW.md` - Detailed setup guide
- `AIRFLOW_QUICK_START.md` - Quick reference
- `TASK4_SUMMARY.md` - This file

All setup is complete and ready to use! ðŸš€

